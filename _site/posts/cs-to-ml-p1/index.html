<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jax Bulbrook">
<meta name="dcterms.date" content="2023-08-21">

<title>Blog - Computer Science to Machine Learning - An intuitive understanding of ML for coders (Part 1)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About Jax Bulbrook</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Computer Science to Machine Learning - An intuitive understanding of ML for coders (Part 1)</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">tutorial</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Jax Bulbrook </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 21, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="summary" class="level1">
<h1>Summary</h1>
<p>This tutorial is meant to give an intuitive explanation of machine learning for those that are already familiar with computer science, specifically Python and OOP. If you aren’t, I would highly suggest learning to code before learning AI, as it will be very challenging otherwise. Check out the amazing <a href="https://100daysofpython.dev/">100 Days of Python Course</a> to do so. But this mini-course assumes no previous knowledge of the field. In fact, I was in the same position roughly a year ago, having gotten interested in ML with the release of ChatGPT. I couldn’t possibly imagine how they are able to program so many different outputs, or how they teach a computer to do it for them. Roughly a year later and my path has deviated several times, as there are so many resources out there it’s hard to keep track. My list of links, videos, and books to read is over 500, and that is unsustainable for someone trying to transition from CS to the newly popular (and high paying) ML. I’m not at all good at math or theory, I prefer using code as an example, but in some cases there is a need to understand the underlying theory, for which I apologize in advance.</p>
</section>
<section id="course-outline" class="level1">
<h1>Course outline</h1>
<p>I am going to be documenting the various ideas on the journey to a career in Machine Learning, so this will probably be ongoing, but for now here is what you will learn:</p>
<ol type="1">
<li>An intuitive understanding of a neural network (this post)</li>
<li>Implementing a simple digit classifier with PyTorch</li>
<li>Behind the scenes: How Backpropogation works</li>
<li>A guide to the confusing aspects of the Transformer</li>
<li>How Stable Diffusion works</li>
<li>Tips to make your model better (dropout, layer norm, nonlinearity, etc)</li>
</ol>
</section>
<section id="what-is-a-neural-network" class="level1">
<h1>What is a neural network?</h1>
<p>Neural networks are simply used to refer to a complex mathematical expression. It is a way to represent any function with a series of “parameters”. For now, you can think of it as a black box, where you pass in information and get out the correct information. This neural network can have it’s complexity increased or decreased, depending on the complexity of the relationship it needs to model. The reason it’s so special, and the reason everyone cares so much about it, is that its a <em>universal function approximator</em>. So it could graph something as simple as x^2 or sin(x), yes, but it can also be expanded to anything you can put into a series of numbers.</p>
<p>So what if you need to figure out if an image is a cat or a dog? Just convert each pixel to a number going from 0 (white) to 1 (black), and feed that in, to get the answer. We could manually figure this out, by writing some incredibly long function with thousands of different variables, but mathematicians would very quickly get tired of that. And how do you figure out what the numbers are? It makes no intuitive sense to us, but computers don’t need intuition. The beauty of a neural network is that you can give it a bunch of data it needs to classify along with the result, and it will figure out how to do that. It does need some way of figuring out if it’s right or wrong, though, or else it would just spit out random answers forever and think it’s getting everything write.</p>
<p>But how does this magical function approximator work? It’s actually modelled after our own brains, which may be why it can now do many of the same things we can do. Here is a simple graph, one that can classify someone as male or female given their height and weight: <img src="simple-nn.png" class="img-fluid" alt="A simple neural net"> It looks a bit confusing at first, but it’s actually very simple. You start with an input layer, with your two numbers you are feeding it (weight and height). There is no need to transform these from our language (words) computer language (numbers) since they already are! Then, you have what’s called a hidden layer. This goes in between the input and output layer, and it is used to give the model a bit more context. If we were only allowed to have one layer, then the weight and height would have to have some number that directly turns them into gender (1 or 0). It’s a bit more complicated than that, so you give the model one (or many) extra layers to process the information a bit better. One is enough, but when you get to image classifiers like ResNet, they have up to 50 layers!<br>
To calculate h1 and h2, which is the data you then pass on as the new inputs to the next layer, you take some weights and a bias. In this case, the math for h1 would be <code>(weight*w1) + (height*w2) + b1</code>. If you aren’t a math person, that’s ok. You are basically taking the weight and height and multiplying them by some number to transform them more towards the number. In this case, the weight is probably in the triple digits, if in pounds, and the height is probably mid 50s in inches. This is a long way away from somehow changing it into a number from 0 to 1 for the gender, so we first need to multiply them by some number. In this case, it would be a very small number like 0.1 or even 0.01, which would turn 100 into 10 or 1 respectively. This is a bit closer to our number, which is why we use weights. Then we need some way to also give it the information from the height, which is calculated in the same way. We can add them together, but that may still not be enough (or too much). We also add a bias, which is just some set number, to make the numbers more standardized. In this case, the weight and height put together would proably go over 1, so the bias could be something like -1 to bring it back to a range of 0-1. And that’s a basic Neural Network!</p>
</section>
<section id="how-would-this-work-in-a-practical-sense" class="level1">
<h1>How would this work in a practical sense?</h1>
<p>We are now going to be introduced into the power of PyTorch, one of many machine learning frameworks for Python. Most of them are roughly the same, so it doesn’t matter what you start with, but this one is most popular in AI research and is growing in careers as well. You can follow along with this tutorial via <a href="https://github.com/Jax-Hax/quarto-blog/blob/main/posts/cs-to-ml-p1/index.ipynb">this Jupyter Notebook</a>. If you don’t know what a Jupyter notebook is, it’s basically a collection of code blocks that you can run 1 by 1 rather than executing a whole script at once. If you don’t have VSCode, Jupyter Lab, or Jupyter Notebook installed on a local computer, you can run this in Kaggle as well, but I won’t cover that here. We are going to be making a simple function classifier, to approximate things like sin(x) and x^2. This will help understand the basics, like datasets, training a neural network, and nonlinearity.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step one, install the required libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pytorch numpy matplotlib <span class="co"># a ! before a line means run in the command line rather than in Python</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt <span class="co"># I had an error and had to also install ipywidgets in case you run into that</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'_mpl-gallery'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Collecting pytorch
  Using cached pytorch-1.0.2.tar.gz (689 bytes)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: numpy in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (1.24.3)
Requirement already satisfied: matplotlib in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (3.7.1)
Requirement already satisfied: fonttools&gt;=4.22.0 in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (from matplotlib) (4.39.3)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (from matplotlib) (1.4.4)
Requirement already satisfied: pyparsing&gt;=2.3.1 in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (from matplotlib) (3.0.9)
Requirement already satisfied: packaging&gt;=20.0 in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (from matplotlib) (23.1)
Requirement already satisfied: pillow&gt;=6.2.0 in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (from matplotlib) (10.0.0)
Requirement already satisfied: contourpy&gt;=1.0.1 in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (from matplotlib) (1.0.7)
Requirement already satisfied: python-dateutil&gt;=2.7 in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (from matplotlib) (2.8.2)
Requirement already satisfied: cycler&gt;=0.10 in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (from matplotlib) (0.11.0)
Requirement already satisfied: six&gt;=1.5 in /home/jaxbulbrook/mambaforge/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)
Building wheels for collected packages: pytorch
  Building wheel for pytorch (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; [6 lines of output]
      Traceback (most recent call last):
        File "&lt;string&gt;", line 2, in &lt;module&gt;
        File "&lt;pip-setuptools-caller&gt;", line 34, in &lt;module&gt;
        File "/tmp/pip-install-vhj1_sxv/pytorch_1774b59f05674eb49275d51a487b1f5c/setup.py", line 15, in &lt;module&gt;
          raise Exception(message)
      Exception: You tried to install "pytorch". The package named for PyTorch is "torch"
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for pytorch
  Running setup.py clean for pytorch
Failed to build pytorch
Installing collected packages: pytorch
  Running setup.py install for pytorch ... error
  error: subprocess-exited-with-error
  
  × Running setup.py install for pytorch did not run successfully.
  │ exit code: 1
  ╰─&gt; [6 lines of output]
      Traceback (most recent call last):
        File "&lt;string&gt;", line 2, in &lt;module&gt;
        File "&lt;pip-setuptools-caller&gt;", line 34, in &lt;module&gt;
        File "/tmp/pip-install-vhj1_sxv/pytorch_1774b59f05674eb49275d51a487b1f5c/setup.py", line 11, in &lt;module&gt;
          raise Exception(message)
      Exception: You tried to install "pytorch". The package named for PyTorch is "torch"
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure

× Encountered error while trying to install package.
╰─&gt; pytorch

note: This is an issue with the package mentioned above, not pip.
hint: See above for output from the failure.</code></pre>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> secret_function(x):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.sin(x) <span class="co"># helper function, this is what the function it will try to approximate will be. You could instead do something like x**2 if you want.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> <span class="op">-</span><span class="dv">15</span> <span class="co"># the start of the graph</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>stop <span class="op">=</span> <span class="dv">15</span> <span class="co"># the end of the graph</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>datapoints <span class="op">=</span> <span class="dv">2000</span> <span class="co"># the number of datapoints on the graph, between start and stop, evenly distributed.</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">2000</span> <span class="co"># epochs are machine learning lingo for how many times it runs through all the data. So you will have a for loop with the number of times, looping through all data each time, resulting in epoch*datapoints number of training examples</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-1</span> <span class="co"># how fast it learns (in scientific notation, 1e-1 is 0.1). This will be explained more in the behind the scenes of how backpropagation works, but basically a higher number means it learns the data quicker but less accurately, and vice versa.</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(start, stop, datapoints).unsqueeze(<span class="dv">1</span>) <span class="co"># linspace and unsqueeze might not look familiar to you, but that's ok. Try looking up what they mean in the Pytorch documentation, but I also have an explanation below.</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> secret_function(x) <span class="co"># the answers to the x's, the same function you defined earlier</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>loss_func <span class="op">=</span> nn.MSELoss() <span class="co"># this is how the model knows what corrections to make. I will explain more in detail below.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="first-what-are-tensors" class="level1">
<h1>First, what are tensors?</h1>
<p>Before we make a neural network, you will need to get this. Tensors sound all big and fancy, but really they are just multidimensional lists. You probably know about dictionaries, where each element has another element it references. This is kind of like that. Each list element is another list element, they are embedded in each other.<br>
Here is a simple example:</p>
<pre><code>tensor1 = [[1,2,3],[4,5,6],[7,8,9]]
or
tensor1 = [
    [1,2,3],
    [4,5,6],
    [7,8,9]
] # same thing</code></pre>
<p>There are some very complicated things you can do with these, but basically it is a way of speeding up your calculations. Instead of running through each training example 1 by 1, which would take thousands of years for something like ChatGPT, you can run through many at the same time on the GPU, by accessing small chunks of the tensor at a time (called batches).<br>
In the above function, torch.linspace(start, stop, datapoints).unsqueeze(1) first makes a tensor with 2000 elements, and they are equally spaced from -15 to 15. Take a look:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>torch.linspace(start, stop, datapoints)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>tensor([-15.0000, -14.9850, -14.9700,  ...,  14.9700,  14.9850,  15.0000])</code></pre>
</div>
</div>
<p>Then, unsqueeze will add another dimension. So all the data will be packed into the first element, in a list. This is because we have one “feature”, or one element we want the data to know about, which is the x. In the example with the height and weight, we would instead have two elements in the array, one for all the weights and one for all the heights. It would then look at each in parallel and be able to see all the data it was given, one in this case. Don’t worry if you don’t get it, it’s quite difficult to grasp and I still am confused constantly by all the different ways of manipulating tensors, like torch.view, @, torch.cat, etc. It will all come with time and practice.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>torch.linspace(start, stop, datapoints).unsqueeze(<span class="dv">1</span>) <span class="co"># notice there are two brackets on the end, because it's a list with only one element.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>tensor([[-15.0000],
        [-14.9850],
        [-14.9700],
        ...,
        [ 14.9700],
        [ 14.9850],
        [ 15.0000]])</code></pre>
</div>
</div>
</section>
<section id="now-how-does-the-actual-model-work" class="level1">
<h1>Now, how does the actual model work?</h1>
<p>In the code example below, we are doing several things, but let me break it down step by step.<br>
1. nn.Sequential. This is basically chaining together a bunch of different things, kind of like a function, where each nn._ is a line to execute. 2. nn.Linear. This is the most basic layer. There are many more, like Conv2d, but this is all we need for now. In our height and weight example, this would be the hidden layer. You first pass in the number of inputs, then the number of outputs. It would be 2,2 for the previous example, but let me show another one.<br>
<img src="adv_network.png" class="img-fluid" alt="More advanced neural net"><br>
This network’s hidden layer had two inputs, and 4 outputs. The input layer is also a linear layer, but it only has outputs, since it IS the inputs. The hidden layer takes it, applies it to each one with different weights and biases, and then gets 4 new numbers, with new information. There is then a final output linear layer (in the code below it would be the final nn.Linear(10,1)), which has 4 inputs and 1 output. Basically this is just PyTorch’s way of representing a vanilla neural network. That’s also the reason we did .unsqueeze, to show that we had one input, rather than 2000, which is the total number of data points.<br>
3. nn.ReLU. This isn’t a layer itself, but it effects the layer before it. It’s called a ReLU Nonlinearity, but really it just breaks up the data. If we didn’t have this, then adding an extra layer wouldn’t actually do anything, because multiplying something by two numbers is the same as just multiplying those two numbers first and only having one layer. Instead, we have to add a little variation. The ReLU is the most basic and also still incredibly efficient one, all it does is set numbers below 0 to 0.<br>
<img src="ReLU.webp" class="img-fluid" alt="Rectified Linear Unit"></p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>), <span class="co"># convert the one input to 10 neurons in a hidden layer</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(), <span class="co"># apply the Rectified Linear Unit (ReLU) nonlinearity, to set negatives to zero</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">50</span>), <span class="co"># convert the 10 neurons to 50 neurons, in another hidden layer. This isn't a necessary step, but it gives the network more depth so it can learn more. Try changing the amount of layers and "parameters" (inputs/outputs) to see if you get a better result</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">50</span>, <span class="dv">10</span>), <span class="co"># bring it back down to 10. I don't think this actually does anything but it looks nice and symmetrical</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>) <span class="co"># convert all the newly learned info into one output. It is possible to get multiple outputs, if you have multiple data points you are shooting for, so for example if you were trying to predict two functions at once. If you are up for a challenge that would be a really good learning opportunity!</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="now-to-train-the-model" class="level1">
<h1>Now to train the model</h1>
<p>Finally, the part you’ve all been waiting for. How do we teach the model to update all these weights? I wouldn’t want to go and try to guess the various parameters myself, and I would hope you wouldn’t either! Our brains do it every day, though, without us opening up our skulls and moving our neurons around. How is that?<br>
It’s because of loss functions. Whenever you do something wrong, and someone is kind enough to tell you constructively what that was, you will know what you need to do next time to do better. That’s what we are trying to do to this little AI, teach it how to predict the function. In this case we are using something called MSE loss, or mean squared error. It is shown below, and don’t worry, I’ll explain it.<br>
<span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]</span> All this fancy math looks pretty complicated! I am quite bad at reading mathematical notation, but luckily after reading the reasoning I think I have the ability to explain it to you concisely. The part in the parenthesis, with the y and the fancy y (called a y hat, since it’s wearing… a hat). The y is simply the actual answer, and y hat is the answer predicted by the neural network. So if it is x^2, and the x was 2, then it would be 4 - whatever the machine predicted. So if it predicted 3, we would say it was 1 off, or if it said 5 it would be 1 too high. The squared then turns it all positive, because the neural network wants to have a low loss, so telling it it was negative would make it pretty happy. This squared also makes bigger losses really bad, and smaller ones not something to worry about, since ^2 enhances numbers. If you instead wanted the smaller errors to count more, you could use mean absolute error, but that’s not used as much.<br>
Finally, we are just summing it up (that’s what the weird E is) and dividing it by the total number of terms, since it adds up when you have a bunch of data points, and that’s not fair to the model. There are several other good to know loss functions, cross entropy is another big one, but I will use that soon enough in a future tutorial! Now here’s the code to do everything:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span>learning_rate) <span class="co"># SGD means stochastic gradient descent, which I will go over more in my backpropogation tutorial.</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass: Compute predicted y by passing x to the model</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    y_preds <span class="op">=</span> model(x)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute and print loss</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_func(y_preds, y) <span class="co"># this is the MSE we defined above</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> t <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">99</span>:</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(t, loss.item()) <span class="co"># just to print out the loss so we can see how it's doing</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform a backward pass and update the weights.</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    loss.backward() <span class="co"># this is how the model knows what weights to update. No need to worry about it for now</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/jaxbulbrook/mambaforge/lib/python3.10/site-packages/torch/autograd/__init__.py:197: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1673730874951/work/c10/cuda/CUDAFunctions.cpp:109.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>99 0.4895337224006653
199 0.48295196890830994
299 0.45047473907470703
399 0.4051359295845032
499 0.3806268572807312
599 0.3800131678581238
699 0.38927873969078064
799 0.3723832368850708
899 0.37137261033058167
999 0.352609246969223
1099 0.366416335105896
1199 0.31635332107543945
1299 0.3125559687614441
1399 0.34680262207984924
1499 0.3333156704902649
1599 0.3620864152908325
1699 0.3848157525062561
1799 0.49152642488479614
1899 0.32129010558128357
1999 0.3082638382911682</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># we can use this function to plot the data with matplotlib and numpy, just to see how it compares</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_plot():</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert tensor data back to numpy arrays for plotting</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    test_x <span class="op">=</span> x.numpy()</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    predicted_y <span class="op">=</span> y_preds.detach().numpy()</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    test_y <span class="op">=</span> y.numpy()</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the results</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    plt.plot(test_x, test_y, <span class="st">"r-"</span>, label<span class="op">=</span><span class="st">'True Function'</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    plt.plot(test_x, predicted_y, label<span class="op">=</span><span class="st">'Predicted Function'</span>, linestyle<span class="op">=</span><span class="st">'dashed'</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>test_plot()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="where-did-we-go-wrong" class="level1">
<h1>Where did we go wrong?</h1>
<p>Why did it flatline? The reality is that with only about a hundred parameters, it couldn’t predict the entire section, so it chose to just be very good at the beginning part. In the future, we could increase the start and stop, add more data points, change the number of layers/parameters, and try a higher/lower learning rate, but this was really just a proof of concept. You now understand most of machine learning! Everything else is really just a fancy way of using a universal function approximator!<br>
If you liked this, that’s great! You have a future in machine learning! If you didn’t, that’s still ok, because you may not really see the practical application yet. I hope you will read the next tutorial, on classifying handwritten digits, for a real world example of how great Machine Learning can be. Imagine writing a program to classify digits all by yourself! Impossible!</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="Jax-Hax/comments-for-blog" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>