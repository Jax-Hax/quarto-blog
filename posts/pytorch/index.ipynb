{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Day 13 - PyTorch: An Imperative Style, High-Performance Deep Learning Library\"\n",
    "author: \"Jax Bulbrook\"\n",
    "date: \"2023-07-30\"\n",
    "categories: [100-papers]\n",
    "---\n",
    "\n",
    "I had an interest in this paper since making the autograd engine from micrograd, since I want to see how they did it. Pytorch was made in C++ to be faster, and is made as an alternative to the traditional compile-based dataflow graph libraries, but also isn't a slow library. It uses things like custom caching tensor allocator to allow for dynamic eager execution, which makes code easier to write for the end user. This has led to the popularization of PyTorch as an alternative to libraries like TensorFlow. This paper actually wasn't released until long after PyTorch was made, since it mentions being cited as the library of choice for many papers already. I use and enjoy PyTorch, so this was a good way to get a behind the scenes look, but it isn't necessary if you just want to do Deep Learning or AI dev."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
