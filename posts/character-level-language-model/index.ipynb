{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Day 10 - Character-Level Language Modeling with Deeper Self-Attention\"\n",
    "author: \"Jax Bulbrook\"\n",
    "date: \"2023-07-27\"\n",
    "categories: [100-papers]\n",
    "---\n",
    "\n",
    "# Summary\n",
    "This was about a character-level model Transformer, replacing the state of the art for character-level modelling compared to RNNs. It was unfortunately still lacking when compared to word-level models, but a good improvement in the character space. It used several state of the art training methods, like momentum, dropout, auxilary losses, positional embeddings, and more.  \n",
    "An interesting point that they pointed out was that when the model saw some text containing elizabeth, it was trained to predict elizabeth in that context, but when they replaced it with a random string of letters, it instantly knew after seeing the first letter and the text that it needed to fill in the rest from the text, rather than using what it was trained on. They did this again for later text, and it ranked the random letter as second right off the bat, unfortunately still picking what it was trained on, but close!\n",
    "# Takeaway\n",
    "Overall this paper wasn't too impressive after previously reading the papers on gpt. It did break the state of the art at the time compared to RNNs, but it still doesn't seem to have a future. I don't think character-level language models have too much promise compared to word-level ones, it does have more flexibility but also requires substantially more training. It did have a good summary of the field of RNNs in the related work section, though, so I may come back to it for that.\n",
    "# Other research\n",
    "I need to do more research into truncated backpropagation through time, sinusoidal timing signals, auxilary losses, and dynamic evaluation (which sounds interesting, a constantly training model maybe?)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
