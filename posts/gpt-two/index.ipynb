{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Day 8 - Language Models are Unsupervised Multitask Learners\"\n",
    "author: \"Jax Bulbrook\"\n",
    "date: \"2023-07-25\"\n",
    "categories: [100-papers]\n",
    "---\n",
    "\n",
    "# Summary\n",
    "This is the GPT-2 paper, an expansion of Day 7's paper on GPT-1. It is a proof of concept demonstrating a model with an order of magnitude more parameters than the original GPT. It was pre-trained on a WebText dataset made by the creators of this paper, which is a collection of every Reddit post with more than 3 upvotes before 2017. They made various models in ordering complexities, and none of them started overfitting, showing that more training time and bigger models is possible even on just Reddit's data. Further improvements like more data (other social media sites, various books/articles, Wikepedia, etc), more training time, bigger models, and fine tuning could be used to increase the performance beyond anything imaginable. Fine-tuning was actually not used in this paper, whereas it was for the previous GPT-1, which means there is another big boost it could use. The whole point, though, was to show that the model does not need to be fine-tuned on various tasks, and the one model is enough to do well on all of them. GPT-1 had to be fine-tuned on many different datasets for different tasks, resulting in more time taken by the researchers, whereas GPT-2 could just be fed the data in a normal way and still perform well. This proof of concept is shown in ChatGPT, which is one model that can be used for almost anything. Overall another great expansion of NLP, OpenAI is really a great research company!\n",
    "\n",
    "# Takeaway\n",
    "Models that take away time from the researchers day are prevalent, but other alternatives are a viable option, using unsupervised training to save time and complexity. This is great, as we may eventually be able to have a universal model that can do everything we need it to, or slowly condense several fields (like question-answer vs translation) into just one (NLP).\n",
    "\n",
    "# Other research\n",
    "Several great research papers in this, as well as some vocab I had never heard of. I actually watched a video on [PCA by StatQuest](https://www.youtube.com/watch?v=FgakZw6K1QQ) this morning, which gave a great explanation, since that was previously a bit confusing to me. I hope to delve more into Statistics in the coming time, and implement some of these papers I am reading. The vocab was: tractable sampling, perplexity, and Bloom filters. I've added the papers to [the github page for this project](https://github.com/Jax-Hax/100-papers-100-days/blob/main/README.md)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
