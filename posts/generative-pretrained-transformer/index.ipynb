{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Day 7 - Improving Language Understanding by Generative Pre-Training\"\n",
    "author: \"Jax Bulbrook\"\n",
    "date: \"2023-07-24\"\n",
    "categories: [100-papers]\n",
    "---\n",
    "\n",
    "# Summary\n",
    "This is the GPT-1 paper, possibly one of the most influential papers of all time. This presented what would later change the world as ChatGPT! It is a Transformer that uses unsupervised pretraining to learn how to understand sentences beforehand, resulting in less training time when they fine-tune it on more specific tasks. This also means there is less need for labelled data to train a model, since any data can be used in an unsupervised way. The actual architecture of the model is a normal Transformer that is first pretrained on non-labelled data, BookCorpus in this case (thousands of unpublished books). This unique dataset allows it to learn long-range information, since they are books not short passages. It is then fine-tuned on different data depending on the task. So for example question answering, the most famous example due to ChatGPT, is done with multiple choice. To do this, they took questions with several answers, and fed each one independently into the model, taking the softmax of the outputs to determine the most probable one. Another interesting thing is that they graphed the performance of the model based off the number of layers, showing that it would benefit from more layers but would overfit faster, and a graph of the number of batches/epochs, which showed that it would benefit from being trained longer. This means that with more data and more layers/training time, it could become much better, which is true as seen in ChatGPT.\n",
    "\n",
    "# Takeaway\n",
    "Unsupervised pretraining looks like a great way to prevent overfitting, I wonder if it can be applied elsewhere. Maybe an approach that starts by training the first layer, then the second layer, then the third, and works it's way up until the final layer would be a nice way to prevent overfitting in the final sensitive layers but work on the first few, but there would need to be loss functions for each individual set of layers. This also shows that data is extremely important, and why ChatGPT was trained on such an incredibly large amount, why it costs millions of dollars to train/run.\n",
    "\n",
    "# Other research\n",
    "There actually wasn't much this time, which is a hopeful time. I used ChatGPT to help me understand the math (ironic), and it was mainly based off the Transformer which I already mostly understand. I would like to implement one from scratch, though, to fully get it. Multi-head attention is still a bit fuzzy, as well as the Q,K, and V matrices. One thing I would like to research further is layernorm vs batchnorm."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
