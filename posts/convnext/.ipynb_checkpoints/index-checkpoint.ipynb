{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e98401-239d-40f4-9dc2-5b5445a9b0b2",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Day 4 - A ConvNet for the 2020s\"\n",
    "author: \"Jax Bulbrook\"\n",
    "date: \"2023-07-21\"\n",
    "categories: [100-papers]\n",
    "---\n",
    "# Summary\n",
    "This paper is an attempt to combine the various state of the art results from various convnets over the years into one model. All have been done before, but never in the same model. This results in a higher performance model than ViT models (Vision Transformers), which were state of the art at the time. The  reason they did this is because ViT models had a few major problems, like a quadratic context length for the image size, meaning it becomes memory intensive quite quickly. To implement the new ConvNeXt, they took Resnet as a base and added various techniques inspired by Swin Transformers. The techniques they used were as follows:  \n",
    "1. Training optimizations such as AdamW optimizer, data augmentation techniques like Mixup, Cutmix, RandAugment, Random Erasing, and regularization scehemes like Stochastic Depth and Label Smoothing. I will have to research all of these topics more in depth. +2.7%\n",
    "2. Macro design: The network as a whole\n",
    "    - Changing stage compute ratio by changing the number of blocks in each stage to (3,3,9,3). The stage compute is how much computing power is dedicated to each stage, so stage 3 has 3x more. +0.6%\n",
    "    - Changing stem to \"Patchify\" - the stem is how the input image is transformed. There is a careful consideration of how much data you really want, because too much is a lot of compute and too little is not enough data. This is done by a 7x7 convolution with stride 2 then a max pool, decreasing the input image by 4x. A patchify layer is a convolution that doesn't overlap, in this case they did 4x4. +0.1%\n",
    "    - Using groups and increasing model size - they use groups, which basically means using many convolutions that each only look at a small portion of the input data, and therefore need fewer parameters. They make many more groups, though, to offset for this loss of performance from only looking at some of the data. +1%\n",
    "    - Using an inverted bottleneck design - I will have to look more into this, but here is my understanding as of right now: You take a 1x1 convolution to exapdn the number of input channels, using a small number of filters called the bottleneck width. They then do a depthwise convolution, adding a seperate filter to each input channel. Finally, they use a 1x1 convolution again the shrink the number of channels to the desired output size. I'm not sure how they are able to shrink it, though, so I'll let you know later. +0.1%\n",
    "    - Using a larger kernel size of 7x7, and putting it after the inverted bottleneck block so that the efficient 1x1 kernels do the heavy lifting while this is inefficient but now run on less data due to the inverted bottleneck shrink. +0\n",
    "3. Micro design: Invidiaul layers\n",
    "    - Replacing ReLU with GELU: ReLU just changes negative numbers to zero, while GELU smoothes them out move. This actually doesn't change accuracy, though. +0\n",
    "    - Fewer activation functions - Transformers only use activation functions after a few blocks. Doing so in this results in +0.7%.\n",
    "    - Fewer normalization layers - They reduce the number of BatchNorm layers by 2. +0.1%\n",
    "    - Substituting BatchNorm with LayerNorm - Layer Normalization is much simpler, (and is added to my research list), resulting in +0.1%.\n",
    "    - Seperate downsampling layers - a downsampling layer decreases the amount of pixels/resolution. This is achieved with a convolution with a stride higher than it's kernel size, so it skips some pixels. They use 2x2 with stride 2. +0.5%  \n",
    "    \n",
    "Overall, this beat the current state of the art by 0.5%, but using an entirely different and much simpler architecture.\n",
    "\n",
    "# Takeaway\n",
    "I wonder if they should combine various techniques from more modern things, like diffusion models or NLP models. Maybe MoE with some other techniques is how they made GPT 4? I guess we shall see as I dive deeper into the ML journey and learn more techniques.\n",
    "\n",
    "# Other research\n",
    "Data augmentation techniques like Mixup, Cutmix, RandAugment, Random Erasing, and regularization scehemes like Stochastic Depth and Label Smoothing. Also study inverted bottleneck and BatchNorm vs LayerNorm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
