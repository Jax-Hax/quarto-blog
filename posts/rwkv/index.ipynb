{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Day 12 - RWKV: Reinventing RNNs for the Transformer Era\"\n",
    "author: \"Jax Bulbrook\"\n",
    "date: \"2023-07-29\"\n",
    "categories: [100-papers]\n",
    "---\n",
    "\n",
    "This paper was a good but complex read. It is about Receptance Weighted Key Value models, which are an alternative to Transformers that combine the ideas from both Transformers and RNNs. It came out only a few months ago, so we are yet to see whether or not it can replace Transformers, but it seems to be a good alternative. It has a much lower time complexity and memory complexity, but the problem is that it may lose the ability to see important information in past tokens, since it does not have the full text at any given time. This is due to \"funneling information through a single vector representation over many time steps\" which allows for less memory usage but also less understanding. Hopefully that isn't a big issue, but I think it may be. If you paste in an entire book and want it to summarize it, it may struggle to look for individual details. I will need to come back to this paper later, as I didn't get all the references and various things they included, as well as the design or math in general. It was kind of the same detail with Transformers, the matrices seem a little fuzzy in my brain. What is the difference between Q, V, and K? I don't know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
