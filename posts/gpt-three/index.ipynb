{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Day 9 - Language Models are Few-Shot Learners\"\n",
    "author: \"Jax Bulbrook\"\n",
    "date: \"2023-07-26\"\n",
    "categories: [100-papers]\n",
    "---\n",
    "\n",
    "# Summary\n",
    "This is the GPT-3 paper, an expansion of Day 8's paper on GPT-2. It is what happens when you take parameters to the extreme. At the largest model, they had 175 billion parameters, and yet it only took a couple cents for inference (training costs were a whole different story). The novel idea they presented was few-shot or zero-shot learning, where you present the model with a few examples of what you want it to do (without updating the weights) and it \"learns\" from those, just in it's context window. This is shown in ChatGPT, where you describe the task and include an example (one-shot learning). This resulted in a model that could match some state of the art models without fine-tuning, which is incredible, since it means there is no need for large fine-tuning datasets. They did preprocess some of the Common Crawl datasets content, as it is just a collection of the Internet, not filtering for high-quality data. They reduced it from dozens of TB of data to only 50 GB. The only architectural difference besides this and GPT-2 is that it uses alternating dense and locally banded sparse attention patterns, which I do not understand at all. I will have to do more research on the Sparse Transformer paper!  \n",
    "The social impacts of this model were also discussed. The model presented a lot of the biases shown on the Internet. An approach to correct this could be filtering text beforehand for sexist or racist content, but we need to find a way to fix this. \n",
    "\n",
    "# Takeaway\n",
    "I think the power of a good dataset is greatly underestimated, and I think OpenAI understands that. Instead of using CommonCrawl, they filtered it into a more high-quality dataset, resulting in better training. I think this could go even further, only including high quality data like books, scientific papers, Wikepedia, and maybe Github (filtered). There is a paper called Textbooks are all you need, which I plan to read, that trains incredibly small models that do well due to the quality of the data they are trained on. If we had 50 GB of good quality data, I think we could get GPT 5 with the same or even less parameters than GPT 3. We could also take a look at all of the tasks GPT 3 is bad on and include small datasets specifically made for them. A math and reversed word dataset would be great for helping GPT understand how the fundamental tokens work. Or they could use a MoE approach, which may be what GPT-4 uses, and train a bunch of smaller language models that are only good at, say, math. This would require training many different models, but it would also result in better performance with less energy usage.\n",
    "\n",
    "# Other research\n",
    "Alternating dense and locally banded sparse attention patterns, beam search, F1 similarity score vs BLEU vs exact match, autoregressive vs bidirectional models, temperature and top p."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
