{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Day 11 - Recurrent Neural Networks (RNNs): A gentle Introduction and Overview\"\n",
    "author: \"Jax Bulbrook\"\n",
    "date: \"2023-07-28\"\n",
    "categories: [100-papers]\n",
    "---\n",
    "\n",
    "# Summary\n",
    "This paper explained several things conceptually, from RNNs to Transformers. This was like the matrix calculus paper, an explanation and not anything new. There was a lot of math, but I actually tried and somewhat succeeded to understand it. I enjoyed it a lot, it presented several things I didn't know about. They first described traditional RNNs, which are basically feed-forward networks but they pass in the weights to the previous layer, allowing for information from past layers to be given to the next ones, basically making them recursive. The way of taking the derivate of this is quite complex, but basically it involves taking the partial derivative of each layer and inputing that for the recursion of each one. This comes with several problems, like vanishing and exploding gradients. There are several ways to fix this, one being long short-term memory, using some weird gates outside of the network to prevent the issues. I don't really get how this works, so I am going to read some of the papers cited here, like the one on StarCraft with LSTMs. There is also bidirectional recurrent neural nets, which are used when you fill in a word in the middle of the sentence, so you need the text both ahead and behind the word. You basically use two seperate recurrent neural nets, one with the normal net and one with the words in reverse, taking the words from back to the blank word.  \n",
    "Then there are encoder-decoder architectures, basically encoding the input into a state and decoding the state. I didn't get it too much from this paper, but I do have a previous understanding from video one of Jeremy Howard's FastAI part 2 course, an intro to stable diffusion. But it seems to allow for encoding into a state that a neural net understands, and then decoding into an output that we understand. They also described attention and neural nets, but I already tried to describe that in my attention is all you need blog post. They did have some good resources that I will try to look into, though. Finally, there are pointer networks, which I didn't get at all, so I have the paper that presented those on my to-read list for tomorrow's paper.\n",
    "\n",
    "# Takeaway\n",
    "Recurrent neural nets seem like a good extension of feed-forward networks, whereas I had previously considered them as extinct due to attention and transformers. It still seems like they could have uses elsewhere, though, and I hope to see them make a return in the future, as no one really understands the Q, K, and V matrices apparently while these are pretty easily understood. I did like all the resources presented in the paper, though, and I will 100% look into those at a later date. I don't usually summarize other blog posts or books, which are the majority of what was presented, but we shall see.\n",
    "\n",
    "# Other research\n",
    "planar convex hulls, Delaunay triangulations, and the symmetric planar Travelling Salesman Problem. I have heard of the Travelling Salesman Problem before from a good [Sebastian Lague video](https://www.youtube.com/watch?v=X-iSQQgOd1A), but I don't know what a symmetric planar one is. I would like to make a math notation cheat sheet at some point to help me, like unions and partial derivates and derivate rules, but it sounds like a lot of work and something like that probably already exists, so maybe I'll try and find it instead. I would like to learn LateX at some point though."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
